{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnK8CQyH52ey3Ai8deEqCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codebizpro/deep_learning_using_neural_networks/blob/main/Deep_Learning_using_Neural_Networks_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# This code will start tracking time, when notebook starts to execute.\n",
        "# In the end we want to evaluate exection time.\n",
        "import time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GNfouTdwZYaj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning:**\n",
        "\n",
        "Deep learning represents the forefront of artificial intelligence, leveraging sophisticated `neural network` models inspired by the human brain to autonomously learn from vast amounts of data, extract intricate patterns, and perform complex tasks with unprecedented accuracy. Its transformative impact spans diverse domains, from image and speech recognition to natural language processing and autonomous driving, fueling innovation and revolutionizing industries worldwide. Enabled by exponential data growth, advancements in computational power, and interdisciplinary collaboration, deep learning continues to evolve rapidly, promising to unlock new frontiers in technology and shape the future of human endeavor."
      ],
      "metadata": {
        "id": "-4gTYPPO9r43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor**:\n",
        "\n",
        "The advent of deep learning has necessitated the development of a new data structure known as `tensors`, tailored specifically to accommodate the unique demands of this revolutionary field. Unlike traditional data types, tensors excel at handling multi-dimensional data, such as `images`, `videos`, and `sequential data`, which are prevalent in deep learning tasks. Their versatility and efficiency in storing and manipulating vast quantities of numerical data make tensors indispensable for representing `inputs`, `weights`, and `activations` within `neural networks`. Moreover, tensors facilitate seamless integration with specialized hardware accelerators like GPUs through frameworks such as PyTorch and TensorFlow, enabling lightning-fast computations crucial for training complex models on massive datasets. Thus, the emergence of tensors as a fundamental data structure underscores their indispensable role in powering the transformative capabilities of deep learning, facilitating advancements across diverse domains and driving innovation on a global scale."
      ],
      "metadata": {
        "id": "I44wO1eJ-QRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch:**\n",
        "\n",
        "PyTorch stands as a pinnacle in the landscape of deep learning frameworks, renowned for its intuitive design, flexibility, and robust performance. Developed by Facebook's AI Research lab (FAIR), PyTorch offers a seamless and dynamic approach to building and deploying neural network models, empowering researchers, engineers, and practitioners to translate groundbreaking ideas into tangible solutions with unparalleled ease. Its dynamic computation graph mechanism facilitates agile experimentation and iterative model development, allowing for real-time adjustments and debugging. Additionally, PyTorch's comprehensive suite of libraries and tools, coupled with its vibrant community support, enables practitioners to tackle a diverse range of tasks, from image and speech recognition to natural language processing and reinforcement learning, with unparalleled efficiency and scalability. As a result, PyTorch has emerged as the framework of choice for cutting-edge research, industrial applications, and educational endeavors, driving innovation and shaping the future of artificial intelligence."
      ],
      "metadata": {
        "id": "DzfbNy_b-jK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Environment"
      ],
      "metadata": {
        "id": "jLWN4BTJEVfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch Package\n",
        "import torch\n",
        "\n",
        "# Import other packages\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "APqRY8WD8hxV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your tensors\n",
        "\n",
        "We can create tensors either from a list or numpy."
      ],
      "metadata": {
        "id": "VJJKqtHGEbQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensor from a list\n",
        "\n",
        "data_01 = [ [1,2,3,4],\n",
        "              [5,6,7,8]]\n",
        "tensor_01 = torch.tensor(data_01)\n",
        "\n",
        "print (tensor_01)\n",
        "\n",
        "# Create tensor from numpy array\n",
        "\n",
        "data_02 = [ [8,2,1,7],\n",
        "              [0,4,6,1]]\n",
        "data_numpy = np.array(data_02)\n",
        "tensor_02 = torch.from_numpy(data_numpy)\n",
        "\n",
        "print (tensor_02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-XXuIGf8hvB",
        "outputId": "a07fafeb-a82f-467a-8369-b2668b75f485"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "tensor([[8, 2, 1, 7],\n",
            "        [0, 4, 6, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Attributes\n",
        "\n",
        "Just like other data structures, Tensors have several attributes like shape and data_type (dtype)\n"
      ],
      "metadata": {
        "id": "9fs5Jy1TExPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor_01.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-SoYyjLFCNO",
        "outputId": "ae2ace05-892e-4aaf-ad83-153cb878178b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (tensor_02.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYK4iK1y8hsk",
        "outputId": "4c507263-d433-468c-f3bf-dc0829691b85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As tensors are optimized for high performance computing. They can either run on 'cpu' or 'gpu'"
      ],
      "metadata": {
        "id": "T-Z1ShclFR3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (tensor_01.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5md4T7rgFe3j",
        "outputId": "df4ec0be-0b16-42d2-86a5-2610796c5226"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (tensor_02.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrsOI1rJJujA",
        "outputId": "c7978144-2da1-4984-9d65-4f5385143cc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Tensors are created for 'cpu'. However, we can always create or copy tensors to run on 'gpu'.\n",
        "\n",
        "For google colabs, we have to manually change the settings of the environment.\n",
        "\n",
        "Step 1: Select Runtime\n",
        "\n",
        "Step 2: Select Change Runtime Type\n",
        "\n",
        "Step 3: Select available GPU under Hardware Accelerator\n",
        "\n",
        "**Warning** Changing the hardware accelerator will delete existing runtime and start a new runtime."
      ],
      "metadata": {
        "id": "JqaotZ04Fifi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using a Nvidia GPU based environment, you can check status of your GPU."
      ],
      "metadata": {
        "id": "cizOSMbHHcyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU information for the runtime\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3D-Je9u8hn3",
        "outputId": "274ffdda-24f2-48da-c82c-919488a5144c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA."
      ],
      "metadata": {
        "id": "tsbdksQbIX_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you just want to know about current GPU device and available cuda cores, you can check this information."
      ],
      "metadata": {
        "id": "44HUPFrgIDpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Available Cuda Cores\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    properties = torch.cuda.get_device_properties(device)\n",
        "\n",
        "    print(\"GPU Name:\", properties.name)\n",
        "    print(\"Number of CUDA Cores:\", properties.multi_processor_count)\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Vd6llS8hlP",
        "outputId": "825271bf-cd19-4de2-abe4-35c015bb9503"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As default Tensor device is 'cpu'. You can change this to 'cuda'. You can also copy existing Tensors to 'cuda' based Tesnors."
      ],
      "metadata": {
        "id": "GFhF5GFKJI6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-1nRcXaKFC9",
        "outputId": "5a3195a4-fda6-4d36-d2aa-49133a91413e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warning** Changing device to 'cuda' does not transfer existing tensors running on 'cpu."
      ],
      "metadata": {
        "id": "3fCzeaWFKr43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (tensor_01.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q74v2hbK1IV",
        "outputId": "fe6f0413-0a30-47c7-9709-7c8823180f09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can copy 'cpu' based Tensor to 'cuda' based tensor, and vice versa."
      ],
      "metadata": {
        "id": "c0UkJmNdL7k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_01 = tensor_01.to(device)\n",
        "print (tensor_01.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwa1fGX4Krd1",
        "outputId": "ed503112-e686-44c0-913f-f62bb9570d83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFeRXr4kTR6D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Operations\n",
        "\n",
        "We can run arithmatic operations between tensors, if they have compatable dimensions.\n",
        "\n",
        "**Warning** Make sure that all the tensors in an operation are using same device."
      ],
      "metadata": {
        "id": "lC3NAhYfL37X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy tensor_02 to 'cuda'\n",
        "\n",
        "tensor_02 = tensor_02.to(device)\n",
        "\n",
        "print (tensor_01 + tensor_02)\n",
        "print (tensor_01 - tensor_02)\n",
        "print (tensor_01 * tensor_02)\n",
        "print (tensor_01 / tensor_02)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWud6z75KrTZ",
        "outputId": "2abe2a10-7c55-4764-a6a8-2688b93770cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9,  4,  4, 11],\n",
            "        [ 5, 10, 13,  9]])\n",
            "tensor([[-7,  0,  2, -3],\n",
            "        [ 5,  2,  1,  7]])\n",
            "tensor([[ 8,  4,  3, 28],\n",
            "        [ 0, 24, 42,  8]])\n",
            "tensor([[0.1250, 1.0000, 3.0000, 0.5714],\n",
            "        [   inf, 1.5000, 1.1667, 8.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Neural Network"
      ],
      "metadata": {
        "id": "zcOyWH_icHtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Image\n",
        "\n",
        "# Display the image of Neural Network\n",
        "Image(url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/250px-Colored_neural_network.svg.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "IGA50r-yc-02",
        "outputId": "0cfc2754-9324-431b-ce4c-fa4f2accec4f",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/250px-Colored_neural_network.svg.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple neural network, with an input layer, a hidden layer and an output layer.\n",
        "\n",
        "Let's build a similar netowrk."
      ],
      "metadata": {
        "id": "b6d3LvvWfqIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import neural network from torch\n",
        "\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "OmW4u6DINppF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input tenser with three features and random values\n",
        "\n",
        "input_tensor = torch.tensor(\n",
        "    [[0.5, 0.4, -0.2]]\n",
        ")\n",
        "\n",
        "print (input_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lek2UpX1Npm3",
        "outputId": "39e190e9-6df7-439c-8038-b082c25c1e4c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5000,  0.4000, -0.2000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a hidden layer with 3 input features and 2 output features\n",
        "\n",
        "hidden_layer = nn.Linear(3, 2)\n"
      ],
      "metadata": {
        "id": "-x_6PYzMNpkQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the output layer\n",
        "\n",
        "output_layer = hidden_layer(input_tensor)\n",
        "\n",
        "print (output_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHRqgfDFh5wI",
        "outputId": "84d9ff72-15fe-4093-d600-12f6c25e8354"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1113, 0.4929]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Linear Layer as Hidden Layer. We can check weights and bias of the layer."
      ],
      "metadata": {
        "id": "UEFHZtfWhcR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (hidden_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKlyw5YVNpiB",
        "outputId": "f4827489-0cae-477c-dfeb-134c3438c5d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0873, -0.5654, -0.1424],\n",
            "        [ 0.3423,  0.5612, -0.1398]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (hidden_layer.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb3qNvmPNpfx",
        "outputId": "55cf33af-f247-42fb-8da0-226fda72b07d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([0.2654, 0.0694], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a neural network layer, the concepts of 'weights' and 'biases' play pivotal roles in shaping the model's ability to learn and make predictions. The **weights represent the strength of connections between neurons** in adjacent layers, essentially determining the impact of input features on the output. Each connection is associated with a weight, which is adjusted during the training process to minimize the difference between predicted and actual outputs. **Biases, on the other hand, represent the offset or baseline value added to the weighted sum of inputs before passing through the activation function**. They allow the model to capture patterns that may not be directly influenced by the input data. Both weights and biases are learnable parameters that are iteratively updated through optimization algorithms like gradient descent, enabling the neural network to gradually improve its performance over time by effectively capturing the underlying patterns in the data."
      ],
      "metadata": {
        "id": "v3_AK5RQic-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch:\n",
        "\n",
        "output = W0 @ input + b0"
      ],
      "metadata": {
        "id": "Wdn0P5LQiz2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general use caess, we use neural networks with multiple layers. We can add as many layers as our use case requires and our hardware supports.\n",
        "\n",
        "These layers are stacked using Sequential class in torch.nn package."
      ],
      "metadata": {
        "id": "-TOXDHAvi8xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a neural network with 8 input features, 2 output features, and 3 hidden layers."
      ],
      "metadata": {
        "id": "5ZoWnDoUjUwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input tensor\n",
        "\n",
        "input_tensor = torch.tensor([[0.5, 0.4, -0.2, 0.33, -0.67, -0.16, 0.55, 0.9]])\n",
        "\n",
        "# Create first hidden layer with 8 input features and 6 output features\n",
        "layer_a = nn.Linear (8, 6)\n",
        "\n",
        "# Create second hidden layer wih 6 input features and 4 output features\n",
        "layer_b = nn.Linear (6,4)\n",
        "\n",
        "# Create third hidden layer with 4 input features and 2 output features\n",
        "layer_c = nn.Linear (4,2)\n",
        "\n",
        "# Stack sequence of layers to create a model\n",
        "\n",
        "model = nn.Sequential (layer_a, layer_b, layer_c)\n",
        "\n",
        "# Check output of the model\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print (output_tensor)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv7bJ7mXNpdh",
        "outputId": "bcb073fa-557b-4949-d924-6ab505ebe013"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3206,  0.2988]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "\n",
        "So far, we have only used linear layer networks.\n",
        "\n",
        "Activation functions in PyTorch are essential components in neural networks that introduce non-linearity, allowing the network to learn complex patterns in data. These functions operate on tensors, which are multi-dimensional arrays representing data in PyTorch. Tensors can be scalars, vectors, matrices, or higher-dimensional arrays, and they form the backbone of computation in PyTorch. Activation functions like `ReLU` (Rectified Linear Unit), `sigmoid`, and `tanh` are applied element-wise to these tensors, transforming their values to introduce non-linearities in the network, facilitating better learning and representation of complex relationships within the data. Overall, activation functions and tensors are fundamental elements in PyTorch for building and training neural networks.\n",
        "\n",
        "Activation functions are used at the output layer or last layer of the network.\n",
        "\n",
        "**Interested to learn more?** Use this Wikipedia link. https://en.wikipedia.org/wiki/Activation_function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAEgorz7PTic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Classification using sigmoid**\n",
        "\n",
        "Choice of Sigmoid function depends upon use case, as it transforms final output of the neural network. One simple example is `binary classification`, and most commonly used activation function is `sigmoid`."
      ],
      "metadata": {
        "id": "Fr2qQYhJRSX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input tensor\n",
        "input_tensor = torch.tensor([[0.3, 0.4, -0.8, 0.6]])\n",
        "\n",
        "# Define 3 linear hidden layers\n",
        "layer_01 = nn.Linear(4,4)\n",
        "layer_02 = nn.Linear(4,2)\n",
        "layer_03 = nn.Linear(2,1)\n",
        "\n",
        "# Define final activation output layer\n",
        "layer_activation = nn.Sigmoid()\n",
        "\n",
        "# Use Sequential to create layerd network\n",
        "model = nn.Sequential (layer_01, layer_02, layer_03, layer_activation)\n",
        "\n",
        "# Use the model for input_tensor transformation\n",
        "output = model(input_tensor)\n",
        "\n",
        "print (output)"
      ],
      "metadata": {
        "id": "bY0qf9AcNpbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da44f4b1-a4e1-4328-e946-f644ac573d67"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3985]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is similar to logistic regression. By defining the threshold value, we can classify the output between class A or class B. Recall famous example of `cat` or `dog` classification on images to use it as analogy for better understanding."
      ],
      "metadata": {
        "id": "_zLCOX6SUzO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun the above code multiple times. You will observe that output is not same. It changes everytime. That is due to the fact that weights and biases are assigned randomly."
      ],
      "metadata": {
        "id": "gX6xRSXOVWTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Class Classification using `softmax`\n",
        "\n",
        "Similar to sigmoid, we can use softmax for multi-class classification.\n",
        "\n",
        "Let us do a classification for 4 classes (4 element vector of output layer)."
      ],
      "metadata": {
        "id": "RImlabDYVwdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input tensor\n",
        "input_tensor = torch.tensor([[0.3, 0.4, -0.8, 0.6]])\n",
        "\n",
        "# Define 3 linear hidden layers\n",
        "layer_01 = nn.Linear(4,6)\n",
        "layer_02 = nn.Linear(6,6)\n",
        "layer_03 = nn.Linear(6,4)\n",
        "\n",
        "# Define final activation output layer\n",
        "layer_activation = nn.Softmax(dim=-1) #dim = -1 indicates that activation function is applied to the last layer\n",
        "\n",
        "# Use Sequential to create layerd network\n",
        "model = nn.Sequential (layer_01, layer_02, layer_03, layer_activation)\n",
        "\n",
        "# Use the model for input_tensor transformation\n",
        "output = model(input_tensor)\n",
        "\n",
        "print (output)"
      ],
      "metadata": {
        "id": "lj72WDjmNpYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7f6867-2c16-4918-a246-aac7ca1f9733"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2519, 0.2039, 0.2457, 0.2985]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of `cat` and `dog` only. This time, we have 2 more classes like `rabbit` and `deer`. Use this analogy to understand above code.\n",
        "\n",
        "**Fun Fact** The outputs are probabilities of the classes. Sum of the output vector is always 1. So, we have a freedom to choose number of output classes."
      ],
      "metadata": {
        "id": "tNAXGbNQW1cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Check endtime to check execution time of the notebook.\n",
        "end_time = time.time()\n",
        "\n",
        "execution_time = end_time - start_time\n",
        "print(\"Total execution time: {:.2f} seconds\".format(execution_time))\n"
      ],
      "metadata": {
        "id": "MXCsIREINpUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "a71ec569-2a0e-4af1-a823-df16a6d8fefc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total execution time: 190.07 seconds\n"
          ]
        }
      ]
    }
  ]
}